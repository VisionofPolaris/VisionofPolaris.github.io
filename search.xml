<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>SRGAN</title>
      <link href="/archives/d5460cb9.html"/>
      <url>/archives/d5460cb9.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>Ledig C, Theis L, Huszár F, et al. Photo-realistic single image super-resolution using a generative adversarial network[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 4681-4690.</p></blockquote><h1 id="解决问题"><a href="#解决问题" class="headerlink" title="解决问题"></a>解决问题</h1><p>通常有监督的超分辨率算法的优化目标是最小化均方差（MSE），同时，也最大化了图像评价指标峰值信噪比（PSNR）。但是，这种评价指标不利于捕捉感知层面的差异（比如图像的纹理细节等），容易使得到的结果过于平滑，缺少高频信息，导致图片看上去不自然。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/29201475/1656943445570-74d8c7d0-7aad-4021-85c9-e5be062b1263.png#clientId=u8df20da6-2601-4&amp;crop=0&amp;crop=0&amp;crop=1&amp;crop=1&amp;from=paste&amp;height=307&amp;id=u34f090cb&amp;margin=%5Bobject%20Object%5D&amp;name=image.png&amp;originHeight=452&amp;originWidth=559&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=true&amp;size=398218&amp;status=done&amp;style=none&amp;taskId=ua33787cc-ee78-4953-9ab9-f79606df28e&amp;title=%E5%B7%A6%E5%9B%BE%E5%85%B7%E6%9C%89%E6%9B%B4%E9%AB%98%E7%9A%84PSNR%EF%BC%8C%E4%BD%86%E5%BE%88%E6%98%8E%E6%98%BE%E5%8F%B3%E5%9B%BE%E7%9A%84%E7%BA%B9%E7%90%86%E7%BB%86%E8%8A%82%E6%9B%B4%E6%B8%85%E6%99%B0&amp;width=379.20001220703125" alt="image.png" title="左图具有更高的PSNR，但很明显右图的纹理细节更清晰"></p><h1 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h1><h2 id="生成器"><a href="#生成器" class="headerlink" title="生成器"></a>生成器</h2><p><img src="https://cdn.nlark.com/yuque/0/2022/png/29201475/1657036461228-180bd094-26eb-4fef-a031-4efb51469f23.png#clientId=u9c7fe7a5-36b9-4&amp;crop=0&amp;crop=0&amp;crop=1&amp;crop=1&amp;from=paste&amp;height=281&amp;id=uaeb164e3&amp;margin=%5Bobject%20Object%5D&amp;name=image.png&amp;originHeight=351&amp;originWidth=1093&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=true&amp;size=160141&amp;status=done&amp;style=none&amp;taskId=u5cb961c6-617f-4cfb-af1a-f697890f1bd&amp;title=%E7%94%9F%E6%88%90%E5%99%A8%E7%BB%93%E6%9E%84&amp;width=874.4" alt="image.png" title="生成器结构"></p><ol><li>作用：由低分辨率图像生成高分辨率图像</li><li>结构：<ol><li>一个卷积层和PReLU函数</li><li>B个残差块</li><li>上采样结构</li></ol></li><li>图片中k代表卷积核尺寸、n代表卷积输出的通道数、s代表步长、箭头表示残差结构、Elementwise Sun是残差中相加的操作</li></ol><h2 id="判别器"><a href="#判别器" class="headerlink" title="判别器"></a>判别器</h2><p><img src="https://cdn.nlark.com/yuque/0/2022/png/29201475/1657037049976-b71fb996-f3e6-4455-801e-e129591ce0fd.png#clientId=u9c7fe7a5-36b9-4&amp;crop=0&amp;crop=0&amp;crop=1&amp;crop=1&amp;from=paste&amp;height=205&amp;id=u190ad174&amp;margin=%5Bobject%20Object%5D&amp;name=image.png&amp;originHeight=256&amp;originWidth=1086&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=true&amp;size=242099&amp;status=done&amp;style=none&amp;taskId=uf16b9a7d-4584-48c3-8116-1ea6dcb445f&amp;title=%E5%88%A4%E5%88%AB%E5%99%A8%E7%BB%93%E6%9E%84&amp;width=868.8" alt="image.png" title="判别器结构"></p><ol><li>作用：判别图片是真实的HR图片还是生成的HR图片</li><li>输出为判断结果，位于0-1之间，越接近1代表判断为真、越接近0代表判断为假</li></ol><h1 id="感知损失函数"><a href="#感知损失函数" class="headerlink" title="感知损失函数"></a>感知损失函数</h1><p>损失函数由判别器损失和生成器损失组成，判别器损失和原始GAN基本一样，生成器损失受感知损失（Perceptual Loss）$l^{SR}$的启发，由两部分组成，即内容损失（Content Loss）和对抗损失（Adversarial Loss）：<br>$$l^{SR}=l_{VGG}^{SR}+10^{-3}l^{SR}_{Gen}$$</p><h2 id="内容损失"><a href="#内容损失" class="headerlink" title="内容损失"></a>内容损失</h2><p>常用的MSE损失函数导致图像缺少高频信息，文章使用VGG损失函数，将生成的HR图像和真实的HR图像送入VGG19网络中的第i个最大池化层前的第j个卷积层之前的网络进行特征提取，然后在提取的特征图上再使用MSE误差：<br>$$l_{VGG/i.j}^{SR} = \frac{1}{W_{i,j}H_{i,j}} \sum_{x=1}^{W_{i,j}} \sum_{y=1}^{H_{i,j}} (\phi_{i,j}(I^{HR})<em>{x,y} - \phi</em>{i,j}(G_{\theta <em>G}(I^{HR}))</em>{x,y})^2$$<br>例如：SRGAN-VGG54代表使用的损失函数为$l_{VGG/5.4}^{SR}$。假定$\phi_{5,4}$是指第五个最大池化前的第四个卷积层之前的网络，即VGG19前16层的网络，其输出用$\phi_{5,4}$(input）表示，输出特征图大小为$W_{i,j} \times H_{i,j} \times C_{i,j}$。</p><h2 id="对抗损失"><a href="#对抗损失" class="headerlink" title="对抗损失"></a>对抗损失</h2><p>与普通GAN的生成器损失基本一样，其中$D_{\theta_D} (G_{\theta_G}(I^{LR}))$代表生成图像判别为真的概率：<br>$$l_{Gen}^{SR} = \sum_{n=1} ^N - \log D_{\theta_D} (G_{\theta_G}(I^{LR}))$$</p><h1 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h1><h2 id="参数设置"><a href="#参数设置" class="headerlink" title="参数设置"></a>参数设置</h2><ol><li>在训练阶段，将HR图像使用bicubic kernel进行下采样和高斯模糊生成LR图片，假设下采样率为r。低分辨率图像大小为$W \times H \times C$，则高分辨率图像为$rW \times rH \times C$。实验中取r为4；</li><li>在ImageNet数据集中随机选取350张图片进行训练，随机裁剪96×96的图片作为HR图片，对应的LR图片大小为24×24；</li><li>使用Adam优化器，$\beta_1$为0.9：<ul><li>SRResNet以10<sup>-4</sup>的学习率进行10<sup>6</sup>次迭代，且使用基于MSE的SRResNet网络进行过预训练</li><li>SRGAN先以10<sup>-4</sup>的学习率进行10<sup>5</sup>次迭代，再以10<sup>-5</sup>的学习率进行另外10<sup>5</sup>次迭代</li></ul></li></ol><h2 id="评价标准"><a href="#评价标准" class="headerlink" title="评价标准"></a>评价标准</h2><ol><li><strong>PSNR</strong>和<strong>SSIM</strong>，使用daala包，在中心裁剪的y通道上计算，并且去除4像素的边界；</li><li>提出了一个新的评价标准<strong>平均意见分数（MOS）</strong>：</li></ol><p>由26个评分人对超分辨率图像进行打分，分数由1（最差质量）-5（最好质量），打分的图片是数据集Set5、Set14和BSD100中原始的HR图像以及对应的LR图像通过以下方法得到的SR图像（带<em>的方法没使用BSD100数据集）：最邻近插值（NN）、双二次插值（bicubic）、SRCNN、SelfExSR、DRCN、ESPCN、SRResNet-MSE、SRResNet-VGG22</em>、SRGAN-MSE<em>、SRGAN-VGG22</em>、SRGAN-VGG54。因此，每个评分人分别对1128个实例进行了评估，并在来自 BSD300训练集的20张图像的NN（得分 1）和HR（得分5）图像上进行了校准。<br><img src="https://cdn.nlark.com/yuque/0/2022/png/29201475/1657286886378-9099b8a4-5f9d-46ac-8640-fc294d3807ac.png#clientId=u0fd90306-a78e-4&amp;crop=0&amp;crop=0&amp;crop=1&amp;crop=1&amp;from=paste&amp;height=216&amp;id=u6f899183&amp;margin=%5Bobject%20Object%5D&amp;name=image.png&amp;originHeight=257&amp;originWidth=596&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=true&amp;size=65701&amp;status=done&amp;style=none&amp;taskId=u9d49937b-6fc7-4b8b-8ccd-6e7bc04fc22&amp;title=BSD100%E4%B8%AD%E5%9B%BE%E7%89%87%E7%9A%84%E6%89%93%E5%88%86%E6%83%85%E5%86%B5%EF%BC%8C%E7%BA%A2%E8%89%B2%E4%B8%BA%E5%B9%B3%E5%9D%87%E5%80%BC&amp;width=501.8000183105469" alt="image.png" title="BSD100中图片的打分情况，红色为平均值"></p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><h2 id="感知损失对比"><a href="#感知损失对比" class="headerlink" title="感知损失对比"></a>感知损失对比</h2><p>使用不同的感知损失在Set5和Set14两个数据集上进行测试：<br><img src="https://cdn.nlark.com/yuque/0/2022/png/29201475/1657288612570-3a4e7652-34ab-41e0-a57e-c1c9238cdbac.png#clientId=u0fd90306-a78e-4&amp;crop=0&amp;crop=0&amp;crop=1&amp;crop=1&amp;from=paste&amp;height=237&amp;id=u30cff921&amp;margin=%5Bobject%20Object%5D&amp;name=image.png&amp;originHeight=296&amp;originWidth=567&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=true&amp;size=74155&amp;status=done&amp;style=none&amp;taskId=uef28a3e9-cef6-4829-b9c5-d06670d4cc2&amp;title=%E6%84%9F%E7%9F%A5%E6%8D%9F%E5%A4%B1%E5%AF%B9%E6%AF%94%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C%EF%BC%881%EF%BC%89&amp;width=453.6" alt="image.png" title="感知损失对比实验结果（1）"><br><img src="https://cdn.nlark.com/yuque/0/2022/png/29201475/1657289021085-552d83aa-f8f1-484f-ad47-67ac295d75d4.png#clientId=u0fd90306-a78e-4&amp;crop=0&amp;crop=0&amp;crop=1&amp;crop=1&amp;from=paste&amp;height=414&amp;id=uacdcce77&amp;margin=%5Bobject%20Object%5D&amp;name=image.png&amp;originHeight=518&amp;originWidth=1119&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=true&amp;size=741773&amp;status=done&amp;style=none&amp;taskId=ua820179c-8508-4799-8438-72b1c0983b7&amp;title=%E6%84%9F%E7%9F%A5%E6%8D%9F%E5%A4%B1%E5%AF%B9%E6%AF%94%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C%EF%BC%882%EF%BC%89&amp;width=895.2" alt="image.png" title="感知损失对比实验结果（2）"></p><ol><li>MSE相对于感知损失具有更高的PSNR和SSIM值，但结果过于平滑，MOS更能反应出真实感觉；</li><li>在Set5上MSE与感知损失差距不大，但在Set14上感知损失的效果明显；</li><li>从图中可以看出，使用VGG更深的网络可以得到更多的纹理细节。</li></ol><h2 id="网络方法对比"><a href="#网络方法对比" class="headerlink" title="网络方法对比"></a>网络方法对比</h2><p>使用SRResNet和SRGAN与其他6种方法进行对比：<br><img src="https://cdn.nlark.com/yuque/0/2022/png/29201475/1657292105385-638f2cab-8bce-45a2-8493-a15eeee7feee.png#clientId=u0fd90306-a78e-4&amp;crop=0&amp;crop=0&amp;crop=1&amp;crop=1&amp;from=paste&amp;height=289&amp;id=u5e974236&amp;margin=%5Bobject%20Object%5D&amp;name=image.png&amp;originHeight=361&amp;originWidth=928&amp;originalType=binary&amp;ratio=1&amp;rotation=0&amp;showTitle=true&amp;size=143219&amp;status=done&amp;style=none&amp;taskId=u2fc5bdff-6634-4878-a6c0-cbf584a5f50&amp;title=%E6%96%B9%E6%B3%95%E5%AF%B9%E6%AF%94%E7%BB%93%E6%9E%9C&amp;width=742.4" alt="image.png" title="方法对比结果"></p><ol><li>SRResNet在PSNR和SSIM上均取得了更好的结果；</li><li>SRGAN具有卓越的感知性能。</li></ol><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本文的创新点在于：</p><ol><li>提出了SRResNet作为生成器的主干网络；</li><li>提出了SRGAN，引入了感知损失以及判别器来提高图片的真实感觉；</li><li>提出了主观的评价标准MOS。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 超分辨率 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/archives/4a17b156.html"/>
      <url>/archives/4a17b156.html</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo generate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
