<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>SRGAN</title>
      <link href="/archives/d5460cb9.html"/>
      <url>/archives/d5460cb9.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>Ledig C, Theis L, Huszár F, et al. Photo-realistic single image super-resolution using a generative adversarial network[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 4681-4690.</p></blockquote><h1 id="解决问题"><a href="#解决问题" class="headerlink" title="解决问题"></a>解决问题</h1><p>通常有监督的超分辨率算法的优化目标是最小化均方差（MSE），同时，也最大化了图像评价指标峰值信噪比（PSNR）。但是，这种评价指标不利于捕捉感知层面的差异（比如图像的纹理细节等），容易使得到的结果过于平滑，缺少高频信息，导致图片看上去不自然。</p><p><img src="http://tva1.sinaimg.cn/large/008cqtC1ly1h414j0twcvj30d50amjwk.jpg" alt="image.png" title="左图具有更高的PSNR，但很明显右图的纹理细节更清晰"></p><h1 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h1><h2 id="生成器"><a href="#生成器" class="headerlink" title="生成器"></a>生成器</h2><p><img src="http://tva1.sinaimg.cn/large/008cqtC1ly1h414m8dagbj30q108dadu.jpg" alt="image.png" title="生成器结构"></p><ol><li>作用：由低分辨率图像生成高分辨率图像</li><li>结构：<ol><li>一个卷积层和PReLU函数</li><li>B个残差块</li><li>上采样结构</li></ol></li><li>图片中k代表卷积核尺寸、n代表卷积输出的通道数、s代表步长、箭头表示残差结构、Elementwise Sun是残差中相加的操作</li></ol><h2 id="判别器"><a href="#判别器" class="headerlink" title="判别器"></a>判别器</h2><p><img src="http://tva1.sinaimg.cn/large/008cqtC1ly1h414msuzb4j30q1065aen.jpg" alt="image.png" title="判别器结构"></p><ol><li>作用：判别图片是真实的HR图片还是生成的HR图片</li><li>输出为判断结果，位于0-1之间，越接近1代表判断为真、越接近0代表判断为假</li></ol><h1 id="感知损失函数"><a href="#感知损失函数" class="headerlink" title="感知损失函数"></a>感知损失函数</h1><p>损失函数由判别器损失和生成器损失组成，判别器损失和原始GAN基本一样，生成器损失受感知损失（Perceptual Loss）$l^{SR}$的启发，由两部分组成，即内容损失（Content Loss）和对抗损失（Adversarial Loss）：<br>$$l^{SR}=l_{VGG}^{SR}+10^{-3}l^{SR}_{Gen}$$</p><h2 id="内容损失"><a href="#内容损失" class="headerlink" title="内容损失"></a>内容损失</h2><p>常用的MSE损失函数导致图像缺少高频信息，文章使用VGG损失函数，将生成的HR图像和真实的HR图像送入VGG19网络中的第i个最大池化层前的第j个卷积层之前的网络进行特征提取，然后在提取的特征图上再使用MSE误差：<br>$$l_{VGG/i.j}^{SR} = \frac{1}{W_{i,j}H_{i,j}} \sum_{x=1}^{W_{i,j}} \sum_{y=1}^{H_{i,j}} (\phi_{i,j}(I^{HR})<em>{x,y} - \phi</em>{i,j}(G_{\theta <em>G}(I^{HR}))</em>{x,y})^2$$<br>例如：SRGAN-VGG54代表使用的损失函数为$l_{VGG/5.4}^{SR}$。假定$\phi_{5,4}$是指第五个最大池化前的第四个卷积层之前的网络，即VGG19前16层的网络，其输出用$\phi_{5,4}$(input）表示，输出特征图大小为$W_{i,j} \times H_{i,j} \times C_{i,j}$。</p><h2 id="对抗损失"><a href="#对抗损失" class="headerlink" title="对抗损失"></a>对抗损失</h2><p>与普通GAN的生成器损失基本一样，其中$D_{\theta_D} (G_{\theta_G}(I^{LR}))$代表生成图像判别为真的概率：<br>$$l_{Gen}^{SR} = \sum_{n=1} ^N - \log D_{\theta_D} (G_{\theta_G}(I^{LR}))$$</p><h1 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h1><h2 id="参数设置"><a href="#参数设置" class="headerlink" title="参数设置"></a>参数设置</h2><ol><li>在训练阶段，将HR图像使用bicubic kernel进行下采样和高斯模糊生成LR图片，假设下采样率为r。低分辨率图像大小为$W \times H \times C$，则高分辨率图像为$rW \times rH \times C$。实验中取r为4；</li><li>在ImageNet数据集中随机选取350张图片进行训练，随机裁剪96×96的图片作为HR图片，对应的LR图片大小为24×24；</li><li>使用Adam优化器，$\beta_1$为0.9：<ul><li>SRResNet以10<sup>-4</sup>的学习率进行10<sup>6</sup>次迭代，且使用基于MSE的SRResNet网络进行过预训练</li><li>SRGAN先以10<sup>-4</sup>的学习率进行10<sup>5</sup>次迭代，再以10<sup>-5</sup>的学习率进行另外10<sup>5</sup>次迭代</li></ul></li></ol><h2 id="评价标准"><a href="#评价标准" class="headerlink" title="评价标准"></a>评价标准</h2><ol><li><strong>PSNR</strong>和<strong>SSIM</strong>，使用daala包，在中心裁剪的y通道上计算，并且去除4像素的边界；</li><li>提出了一个新的评价标准<strong>平均意见分数（MOS）</strong>：</li></ol><p>由26个评分人对超分辨率图像进行打分，分数由1（最差质量）-5（最好质量），打分的图片是数据集Set5、Set14和BSD100中原始的HR图像以及对应的LR图像通过以下方法得到的SR图像（带<em>的方法没使用BSD100数据集）：最邻近插值（NN）、双二次插值（bicubic）、SRCNN、SelfExSR、DRCN、ESPCN、SRResNet-MSE、SRResNet-VGG22</em>、SRGAN-MSE<em>、SRGAN-VGG22</em>、SRGAN-VGG54。因此，每个评分人分别对1128个实例进行了评估，并在来自 BSD300训练集的20张图像的NN（得分 1）和HR（得分5）图像上进行了校准。<br><img src="http://tva1.sinaimg.cn/large/008cqtC1ly1h414n8qtrpj30gk075dh3.jpg" alt="image.png" title="BSD100中图片的打分情况，红色为平均值"></p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><h2 id="感知损失对比"><a href="#感知损失对比" class="headerlink" title="感知损失对比"></a>感知损失对比</h2><p>使用不同的感知损失在Set5和Set14两个数据集上进行测试：<br><img src="http://tva1.sinaimg.cn/large/008cqtC1ly1h414nno6nnj30fr088abd.jpg" alt="image.png" title="感知损失对比实验结果（1）"><br><img src="http://tva1.sinaimg.cn/large/008cqtC1ly1h414nyibraj30q10c2gxa.jpg" alt="image.png" title="感知损失对比实验结果（2）"></p><ol><li>MSE相对于感知损失具有更高的PSNR和SSIM值，但结果过于平滑，MOS更能反应出真实感觉；</li><li>在Set5上MSE与感知损失差距不大，但在Set14上感知损失的效果明显；</li><li>从图中可以看出，使用VGG更深的网络可以得到更多的纹理细节。</li></ol><h2 id="网络方法对比"><a href="#网络方法对比" class="headerlink" title="网络方法对比"></a>网络方法对比</h2><p>使用SRResNet和SRGAN与其他6种方法进行对比：<br><img src="http://tva1.sinaimg.cn/large/008cqtC1ly1h414obnek7j30pr0a1acy.jpg" alt="image.png" title="方法对比结果"></p><ol><li>SRResNet在PSNR和SSIM上均取得了更好的结果；</li><li>SRGAN具有卓越的感知性能。</li></ol><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本文的创新点在于：</p><ol><li>提出了SRResNet作为生成器的主干网络；</li><li>提出了SRGAN，引入了感知损失以及判别器来提高图片的真实感觉；</li><li>提出了主观的评价标准MOS。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 超分辨率 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/archives/4a17b156.html"/>
      <url>/archives/4a17b156.html</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo generate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
